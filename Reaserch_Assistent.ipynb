{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1cLtafodRMbl3oKDPxclIBpe0Q3_tHm4w",
      "authorship_tag": "ABX9TyOAlj4rAvwWgx8Jsgt5fODg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ranabilal09/Reaserch-Assistent/blob/main/Reaserch_Assistent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fK6ZX_GF7wHx",
        "outputId": "fe42ef31-696f-4768-c746-a1615f6e1335"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'Reaserch-Assistent' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "!git clone  https://github.com/ranabilal09/Reaserch-Assistent.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain langgraph langchain-ollama langchain_community langchain-google-genai langserve arxiv duckduckgo-search"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q9WO84qiuqCy",
        "outputId": "b55712a9-8e6f-443d-d6ff-8998248f4fa0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.3.9)\n",
            "Collecting langgraph\n",
            "  Downloading langgraph-0.2.59-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting langchain-ollama\n",
            "  Downloading langchain_ollama-0.2.1-py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting langchain_community\n",
            "  Downloading langchain_community-0.3.11-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting langchain-google-genai\n",
            "  Downloading langchain_google_genai-2.0.7-py3-none-any.whl.metadata (3.6 kB)\n",
            "Collecting langserve\n",
            "  Downloading langserve-0.3.0-py3-none-any.whl.metadata (39 kB)\n",
            "Collecting arxiv\n",
            "  Downloading arxiv-2.1.3-py3-none-any.whl.metadata (6.1 kB)\n",
            "Collecting duckduckgo-search\n",
            "  Downloading duckduckgo_search-6.4.1-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.36)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.11.9)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.21 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.3.21)\n",
            "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.3.2)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.147)\n",
            "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.26.4)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.10.3)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (9.0.0)\n",
            "Collecting langchain-core<0.4.0,>=0.3.21 (from langchain)\n",
            "  Downloading langchain_core-0.3.24-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting langgraph-checkpoint<3.0.0,>=2.0.4 (from langgraph)\n",
            "  Downloading langgraph_checkpoint-2.0.8-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting langgraph-sdk<0.2.0,>=0.1.42 (from langgraph)\n",
            "  Downloading langgraph_sdk-0.1.43-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting ollama<1,>=0.3.0 (from langchain-ollama)\n",
            "  Downloading ollama-0.4.4-py3-none-any.whl.metadata (4.7 kB)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain_community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting httpx-sse<0.5.0,>=0.4.0 (from langchain_community)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting langchain\n",
            "  Downloading langchain-0.3.11-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain_community)\n",
            "  Downloading pydantic_settings-2.6.1-py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting filetype<2.0.0,>=1.2.0 (from langchain-google-genai)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: google-generativeai<0.9.0,>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from langchain-google-genai) (0.8.3)\n",
            "Requirement already satisfied: httpx<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langserve) (0.28.0)\n",
            "Requirement already satisfied: orjson<4,>=2 in /usr/local/lib/python3.10/dist-packages (from langserve) (3.10.12)\n",
            "Collecting feedparser~=6.0.10 (from arxiv)\n",
            "  Downloading feedparser-6.0.11-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: click>=8.1.7 in /usr/local/lib/python3.10/dist-packages (from duckduckgo-search) (8.1.7)\n",
            "Collecting primp>=0.8.3 (from duckduckgo-search)\n",
            "  Downloading primp-0.8.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.18.3)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading marshmallow-3.23.1-py3-none-any.whl.metadata (7.5 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting sgmllib3k (from feedparser~=6.0.10->arxiv)\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.10 in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (0.6.10)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (2.19.2)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (2.151.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (2.27.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (4.25.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (4.66.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (4.12.2)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from google-ai-generativelanguage==0.6.10->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (1.25.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1.0,>=0.23.0->langserve) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1.0,>=0.23.0->langserve) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1.0,>=0.23.0->langserve) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx<1.0,>=0.23.0->langserve) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1.0,>=0.23.0->langserve) (0.14.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.21->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.21->langchain) (24.2)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from langgraph-checkpoint<3.0.0,>=2.0.4->langgraph) (1.1.0)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (1.0.0)\n",
            "Collecting httpx<1.0,>=0.23.0 (from langserve)\n",
            "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<1.0,>=0.23.0->langserve) (1.3.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.1)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain_community)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.4.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.2.3)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (1.66.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (4.9)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.21->langchain) (3.0.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1.0,>=0.23.0->langserve) (1.2.2)\n",
            "Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (0.22.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (0.2.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (4.1.1)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.10->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (1.68.1)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.10->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (1.62.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.10/dist-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (3.2.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (0.6.1)\n",
            "Downloading langgraph-0.2.59-py3-none-any.whl (135 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.8/135.8 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_ollama-0.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading langchain_community-0.3.11-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m50.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain-0.3.11-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_google_genai-2.0.7-py3-none-any.whl (41 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langserve-0.3.0-py3-none-any.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m28.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading arxiv-2.1.3-py3-none-any.whl (11 kB)\n",
            "Downloading duckduckgo_search-6.4.1-py3-none-any.whl (39 kB)\n",
            "Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading feedparser-6.0.11-py3-none-any.whl (81 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading langchain_core-0.3.24-py3-none-any.whl (410 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph_checkpoint-2.0.8-py3-none-any.whl (35 kB)\n",
            "Downloading langgraph_sdk-0.1.43-py3-none-any.whl (31 kB)\n",
            "Downloading ollama-0.4.4-py3-none-any.whl (13 kB)\n",
            "Downloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading primp-0.8.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m47.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_settings-2.6.1-py3-none-any.whl (28 kB)\n",
            "Downloading marshmallow-3.23.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Building wheels for collected packages: sgmllib3k\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6047 sha256=ea4a52c39aa33bc53836f081be8103ee760bf6468c12c6c50c9adab8f038e997\n",
            "  Stored in directory: /root/.cache/pip/wheels/f0/69/93/a47e9d621be168e9e33c7ce60524393c0b92ae83cf6c6e89c5\n",
            "Successfully built sgmllib3k\n",
            "Installing collected packages: sgmllib3k, filetype, python-dotenv, primp, mypy-extensions, marshmallow, httpx-sse, feedparser, typing-inspect, httpx, duckduckgo-search, arxiv, pydantic-settings, ollama, langgraph-sdk, dataclasses-json, langchain-core, langserve, langgraph-checkpoint, langchain-ollama, langgraph, langchain, langchain-google-genai, langchain_community\n",
            "  Attempting uninstall: httpx\n",
            "    Found existing installation: httpx 0.28.0\n",
            "    Uninstalling httpx-0.28.0:\n",
            "      Successfully uninstalled httpx-0.28.0\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 0.3.21\n",
            "    Uninstalling langchain-core-0.3.21:\n",
            "      Successfully uninstalled langchain-core-0.3.21\n",
            "  Attempting uninstall: langchain\n",
            "    Found existing installation: langchain 0.3.9\n",
            "    Uninstalling langchain-0.3.9:\n",
            "      Successfully uninstalled langchain-0.3.9\n",
            "Successfully installed arxiv-2.1.3 dataclasses-json-0.6.7 duckduckgo-search-6.4.1 feedparser-6.0.11 filetype-1.2.0 httpx-0.27.2 httpx-sse-0.4.0 langchain-0.3.11 langchain-core-0.3.24 langchain-google-genai-2.0.7 langchain-ollama-0.2.1 langchain_community-0.3.11 langgraph-0.2.59 langgraph-checkpoint-2.0.8 langgraph-sdk-0.1.43 langserve-0.3.0 marshmallow-3.23.1 mypy-extensions-1.0.0 ollama-0.4.4 primp-0.8.3 pydantic-settings-2.6.1 python-dotenv-1.0.1 sgmllib3k-1.0.0 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata , drive\n",
        "import os\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "api = userdata.get('Gemini_Api_Key')\n",
        "os.environ['GOOGLE_API_KEY'] = api\n",
        "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = userdata.get('langchai_api_key')\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = 'Research-Assistent'"
      ],
      "metadata": {
        "id": "ca20kNWc-gfe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa930252-456f-46a4-de07-dd4548e1d8f9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "from langchain.schema.runnable import RunnablePassthrough\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import json\n",
        "from langchain.utilities import DuckDuckGoSearchAPIWrapper\n",
        "\n",
        "ddg_search = DuckDuckGoSearchAPIWrapper()\n",
        "\n",
        "def web_serarch(query:str , num_results:int = 3):\n",
        "  results = ddg_search.results(query , num_results)\n",
        "  return [r[\"link\"] for r in results]\n",
        "\n",
        "\n",
        "\n",
        "#prompt\n",
        "Template = \"\"\" {text}\n",
        "\n",
        "-------------------------\n",
        "\n",
        "Using the above context , answer in short the following question:\n",
        "\n",
        "> {question}\n",
        "\n",
        "If the question cannot be anwsered using the text , imply summarize the text. Include all the factual information , numbers and states etc, if available. \"\"\"\n",
        "summary_prompt = ChatPromptTemplate.from_template(Template)\n",
        "\n",
        "url = \"https://blog.langchain.dev/announcing-langsmith/\"\n",
        "\n",
        "def scrape_url(url):\n",
        "  try:\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "      soup = BeautifulSoup(response.text, 'html.parser')\n",
        "      page_text = soup.get_text(separator=\" \" , strip = True)\n",
        "      return page_text\n",
        "    else:\n",
        "      return f\"failed to retrive the websearch , request status code:{response.status_code}\"\n",
        "  except Exception as e:\n",
        "    print(e)\n",
        "    return f\"Failed to retrieve the web search , An error occurred: {str(e)}\"\n",
        "\n",
        "scrape_summarize_Chain = RunnablePassthrough.assign(\n",
        "    summary=RunnablePassthrough.assign(\n",
        "    text = lambda x: scrape_url(x[\"url\"])[:10000]\n",
        ") | summary_prompt | ChatGoogleGenerativeAI(model= \"gemini-1.5-flash-8b\") | StrOutputParser()\n",
        ") | (lambda x: f\"URL:{x['url']}\\n\\n summary:{x['summary']}\")\n",
        "\n",
        "web_serarch_chain = RunnablePassthrough.assign(\n",
        "    urls = lambda x: web_serarch(x[\"question\"])\n",
        ") | (lambda x: [{\"question\":x[\"question\"],\"url\":url}for url in x[\"urls\"]]) | scrape_summarize_Chain.map()\n",
        "\n",
        "# #arxiv\n",
        "# from langchain_community.retrievers import ArxivRetriever\n",
        "\n",
        "# retriever = ArxivRetriever()\n",
        "\n",
        "# #prompt\n",
        "# Template = \"\"\" {docs}\n",
        "\n",
        "# -------------------------\n",
        "\n",
        "# Using the above context , answer in short the following question:\n",
        "\n",
        "# > {question}\n",
        "\n",
        "# If the question cannot be anwsered using the text , imply summarize the text. Include all the factual information , numbers and states etc, if available. \"\"\"\n",
        "# Summary_prompt = ChatPromptTemplate.from_template(Template)\n",
        "\n",
        "# scrape_summarize_Chain = RunnablePassthrough.assign(\n",
        "#     summary= Summary_prompt | ChatGoogleGenerativeAI(model= \"gemini-1.5-flash-8b\") | StrOutputParser()\n",
        "# ) | (lambda x: f\"Title:{x['docs'].metadata['Title']}\\n\\n summary:{x['summary']}\")\n",
        "\n",
        "# web_serarch_chain = RunnablePassthrough.assign(\n",
        "#     docs = lambda x: retriever.get_summaries_as_docs(x[\"question\"])\n",
        "# ) | (lambda x: [{\"question\":x[\"question\"],\"docs\":doc}for doc in x[\"docs\"]]) | scrape_summarize_Chain.map()\n",
        "\n",
        "system_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"user\",\n",
        "         \"Write 3 google search queries to serch online that form an\"\n",
        "         \"objective opinion from the following:{question}\"\n",
        "         \"you must response in the list of string in the following pattern:\"\n",
        "         '[\"query1\",\"query2\",\"query3\"]')\n",
        "    ]\n",
        ")\n",
        "\n",
        "system_question_chain = system_prompt | ChatGoogleGenerativeAI(model= \"gemini-1.5-flash-8b\") | StrOutputParser() | json.loads\n",
        "\n",
        "full_research_chain = system_question_chain | (lambda x:[{\"question\":q}for q in x]) | web_serarch_chain.map()\n"
      ],
      "metadata": {
        "id": "n8TYs-Xo_MS6"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#final prompt\n",
        "system_template = \"You are AI critical thinker research assistent.Your sole purpose is to write well written , critically acclaimed, objective and structured reports on given text.\"\n",
        "user_template = \"\"\"Information:\n",
        "--------\n",
        "{research_summary}\n",
        "--------\n",
        "Using the above information, answer the following question or topic: \"{question}\" in a detailed report -- \\\n",
        "The report should focus on the answer to the question, should be well structured, informative, \\\n",
        "in depth, with facts and numbers if available and a minimum of 1,200 words.\n",
        "You should strive to write the report as long as you can using all relevant and necessary information provided.\n",
        "You must write the report with markdown syntax.\n",
        "You MUST determine your own concrete and valid opinion based on the given information. Do NOT deter to general and meaningless conclusions.\n",
        "Write all used source urls at the end of the report, and make sure to not add duplicated sources, but only one reference for each.\n",
        "You must write the report in apa format.\n",
        "Please do your best, this is very important to my career.\"\"\"\n",
        "\n",
        "writter_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\",system_template),\n",
        "    (\"user\", user_template)\n",
        "])\n",
        "\n",
        "def collapse_list_of_lists(list_of_lists):\n",
        "    flat_list = []\n",
        "    for sublist in list_of_lists:\n",
        "        flat_list.extend(\"\\n\\n\".join(sublist))\n",
        "    return flat_list\n",
        "\n",
        "chain = RunnablePassthrough.assign(\n",
        "    research_summary= full_research_chain | collapse_list_of_lists\n",
        ") | writter_prompt | ChatGoogleGenerativeAI(model= \"gemini-1.5-flash-8b\") | StrOutputParser()\n",
        "\n"
      ],
      "metadata": {
        "id": "Ik1fqCJ1fliX"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "G0wgY9mMhg_a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chain.invoke({\"question\":\"Who is typically older: point guards and centers?\"})"
      ],
      "metadata": {
        "id": "lhkNs6Ev1LWW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "92c10f9c-ead6-4112-fb64-f5d86824a4fd"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"# NBA Player Age Comparison: Point Guards vs. Centers\\n\\n**Executive Summary:**\\n\\nBased on the provided text snippets, centers are generally older than point guards in the NBA.  While the data does not present a comprehensive statistical comparison of average ages across all seasons, multiple sources highlight the trend of centers having a higher average age than point guards. This difference in age distribution likely reflects the distinct physical demands and skill sets required for each position.\\n\\n**Introduction:**\\n\\nThis report analyzes the provided text snippets to determine whether point guards or centers tend to be older in the NBA.  The information focuses on various aspects of NBA player demographics, but a direct comparison of average ages between point guards and centers is not always explicitly stated.  The report will synthesize the available information to draw a conclusion about the typical age difference between these two key positions.\\n\\n**Data Analysis and Findings:**\\n\\nThe first text snippet from jokermag.com provides a glimpse into the average ages of point guards and centers. The average age of an NBA point guard is reported as 25.77 years, while the average age of an NBA center is 27.13 years. This initial data suggests a notable difference, with centers being approximately 1.36 years older, on average, than point guards.\\n\\nHowever, subsequent snippets from nba.com and other sources do not directly address the average age difference between point guards and centers. Instead, these sources provide data on player heights, weights, rookie ages, team experience, and historical data on specific players.  This information, while valuable for understanding the overall landscape of NBA player demographics, does not provide the precise average age comparison needed to definitively answer the question.\\n\\n\\n**Discussion and Interpretation:**\\n\\nThe available information indicates a likely, but not definitively proven, pattern of centers being older than point guards.  The initial data point from jokermag.com strongly suggests this difference. However, the absence of a consistent, across-the-board comparison of ages for both positions across multiple seasons limits the scope of analysis.\\n\\nSeveral factors might contribute to this potential age difference. Centers often require a greater degree of physical maturity and strength to perform their roles.  The rigorous physical demands, including the need for superior strength and agility to rebound and protect the basket, may influence player longevity.  The demands of the center position often result in greater wear and tear on the body, potentially leading to earlier career endings compared to point guards.\\n\\nConversely, point guards, with their emphasis on speed, quickness, and decision-making, might experience less physical stress, potentially extending their careers.\\n\\n**Potential Explanatory Factors:**\\n\\n* **Physical Demands:** The inherent physical differences between the positions. Centers face greater physical strain due to their role in rebounding, blocking shots, and playing inside.\\n* **Injury Rates:** Potential higher rates of injury for centers due to the physical nature of the position.\\n* **Skill Sets:**  Different skill sets might influence longevity.  Point guards' reliance on quickness and agility might contribute to longer careers.\\n* **Career Trajectory:** The paths of point guards and centers might differ. Centers might be more likely to specialize in their position, while point guards might transition to different roles if their skills decline or if injuries occur.\\n\\n**Limitations of the Data:**\\n\\nThe available data is fragmented and not focused specifically on comparing the average age of point guards versus centers. The snippets provide information on various aspects of NBA player demographics, but a direct, quantitative comparison for average ages across different seasons is absent.  Furthermore, the lack of a consistent methodology across the various sources makes a definitive comparison difficult.\\n\\n**Conclusion:**\\n\\nWhile the provided information strongly suggests that centers tend to be older than point guards in the NBA, a conclusive answer requires a comprehensive dataset with consistent data collection methods for comparison across different seasons.  The initial data point supports the hypothesis, but further analysis with more comprehensive, longitudinal data is necessary to confirm this pattern.\\n\\n**Recommendations:**\\n\\nTo provide a more accurate and comprehensive analysis, future research should focus on:\\n\\n* **Collecting longitudinal data:** Gathering consistent data on average player ages for point guards and centers across multiple seasons.\\n* **Standardized methodology:** Implementing a standardized methodology for collecting and analyzing player age data.\\n* **Detailed position analysis:** Analyzing the specific factors that might influence the longevity of point guards and centers, such as injury rates and career trajectories.\\n\\n**Opinion:**\\n\\nBased on the available data, the provided information supports the conclusion that centers are generally older than point guards, but further research is needed to validate this observation.  The initial data point from jokermag.com provides a strong initial indication, but this needs to be supported by data from multiple sources across a significant period of time.\\n\\n\\n**References:**\\n\\n* [jokermag.com URL (not provided in prompt)]\\n* [nba.com URL (not provided in prompt)]\\n\\n\\n**Note:** The URLs were not provided in the prompt, so I cannot cite them correctly.  Please provide the URLs to allow for proper APA formatting.\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install --upgrade langserve fastapi uvicorn sse_starlette python-dotenv"
      ],
      "metadata": {
        "id": "VOK3E01Hr93L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from fastapi import FastAPI\n",
        "import uvicorn\n",
        "from langserve import add_routes\n",
        "\n",
        "import nest_asyncio\n",
        "\n",
        "# This is the crucial fix for Jupyter/Colab environments\n",
        "nest_asyncio.apply()\n",
        "\n",
        "app = FastAPI(\n",
        "  title=\"LangChain Server\",\n",
        "  version=\"1.0\",\n",
        "  description=\"A simple api server using Langchain's Runnable interfaces\",\n",
        ")\n",
        "\n",
        "add_routes(app, chain, path=\"/research\")\n",
        "\n",
        "if  __name__ == \"__main__\":\n",
        "  uvicorn.run(app,host=\"localhost\", port=5000)"
      ],
      "metadata": {
        "id": "ULfSDwGVqhRA",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}